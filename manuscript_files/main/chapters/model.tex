\section{Model of the System}

This section introduces the model of the elevator system studied here. The modelling approach is similar to the one used by \cite{walczak2006, crites_barto_1998}. We use this model in all further analysis.

\subsection{System Dynamics}
The system dynamics are parameterized as follows (as in \cite{pepyne_96}):

\begin{itemize}
    \item Number of floors: 5.
    \item Number of elevator cars: 1.
    \item Elevator maximum speed: 2.54 m/s.
    \item Elevator floor time (time it takes for an elevator to pass a floor at full speed): 1.45 seconds.
    \item Elevator acceleration time (time it takes for an elevator accelerate from a stop to full speed or decelerate from full speed to a stop): 3.595 seconds.
    \item Elevator load time (the time it takes for a passenger to enter or exit the elevator): 1 second.
    \item Floor height: 3.66 meters.
    \item Capacity of the cars: 20 passengers.
\end{itemize}

Passengers can arrive on any floor, where they can press either the up or down button. We call this a \textbf{hall call}. After an elevator arrives at the passenger's floor, they get in and press a button to a floor in the same direction as the elevator. We call this a \textbf{car call}.

An elevator system can be very complex, with events such as passenger and elevator arrivals happening asynchronously. Even though time is continuous, it is still possible to model the elevator system as a discrete event system, where significant events happen at discrete times, with the time between them taking on a continuous value. In this case, the constant discount factor $\gamma$ used in most discrete-time reinforcement learning algorithms is inappropriate. Instead, we take into account the amount of time between events using a time varying discount factor \cite{bradtke_94}. This is also done to take into account the asynchronicity of events that is well suited to the elevator model. We define returns as the total discounted reward received over time:
\begin{equation}
    \int\limits_0^{\infty} e^{-\beta \tau} r_{\tau} d\tau
\end{equation}

where we integrate over continuous time, instead of the usual discrete-time reinforcement:

\begin{equation*}
    \sum_{t=0}^{\infty} \gamma^t r_t.
\end{equation*}

Here, $r_t$ is the immediate cost in the system at discrete time $t$, $r_\tau$ is the instantaneous cost at continuous time $\tau$ and $\beta > 0$ is a parameter used to control the rate of exponential decay. A system with a higher $\beta$ cares more about rewards that are closer in time. The environment is thusly modeled as a discrete-time Markov pecision process with a finite number of states and actions. Using this approach, the states and actions can be fully specified.

The model parameters have been chosen such that the state space is finite and reasonably sized. This is of great importance when using algorithms in which it is necessary to store values for each state, so called tabular methods. If the state space is too large, running time and memory capacity can become intractable. We would need to approximate the value functions. 

% We use several assumptions to simplify the model further. We assume that the maximum number of passengers waiting for an elevator is 2. If another passenger arrives at a floor where 2 passengers are already waiting, we treat it as if there are still only 2 passengers waiting. We do this to limit the size of the state space.

It is not possible to observe the full state of the system. After a button is pressed, the elevator controller does not know if another passenger arrived after the first one. One way to deal with this is assuming \textit{omniscience}. We assume that in every state, the elevator controller knows how many passengers are waiting at a floor, when they arrived and where they are going. This can then be used to calculate the reinforcement signals. An important thing to note here is that it is not the controller that is receiving this extra information, it is the critic \textit{evaluating} the controller. This information is only important in the training phase. Once the controller is trained, it can be implemented in a real system without needing the extra knowledge.

Another possibility is to let the system learn using only information that would be available to the system on-line. The arrival times of passengers after the first would have to be estimated, so that expected costs can still be computed. We use omniscient reinforcements, since they are slightly more accurate and results will not differ by a large margin \cite{crites_barto_group_1998}.

\subsection{State Space}

An elevator controller maps state information to decisions. The definition of the state is therefore of great importance. It needs to properly reflect the current state of the system, measured by quantities that can be practically observed. This includes:

\begin{itemize}
    \item the state of hall calls and their waiting times,
    \item awaiting car calls and their waiting times for all elevators,
    \item position of all elevators,
    \item moving direction of all elevators,
    \item velocity of all elevators,
    \item calls to which particular elevators are allocated.
\end{itemize}

If we assume the state is defined as above, it is possible to estimate the size of the resulting state space. Let $n$ be the number of floors. The size of the state space can be estimated as follows:

\begin{itemize}
    \item at most $n - 1$ up hall calls: $2^{n-1}$ possibilities,
    \item at most $n - 1$ down hall calls: $2^{n-1}$ possibilities,
    \item at most $n$ car calls for each elevator: $2^{n}$ possibilities,
    \item possible locations for each elevator: $n$ possibilities,
    \item possible elevator directions for each elevator: $3$ possibilities (up, down or stopped),
\end{itemize}

which gives a state space size of $2^{n - 1} \cdot 2^{n - 1} \cdot 2^{n} \cdot n \cdot 3$ in a situation where each elevator is considered as a seperate learner. The amount of states rises tremendously in the number of floors. The problem already gets computationally intractable for a relatively few number of floors. To combat this and further reduce the size of the state space, we aggregate the states into a more compact representation, while hopefully keeping the most relevant information. Instead of receiving information regarding exactly which hall and car calls are active, the controller only receives: 

\begin{itemize}
    \item the number of up and down awaiting hall calls higher and lower than the elevator's current position,

    \item the number of car calls to floors in the current moving direction.
\end{itemize}

Taking into account this particular aggregation of states, we can recalculate the size of the state space as follows:

\begin{itemize}
    \item the number of remaining \textbf{up} hall calls from floors \textbf{higher} than the current position: at most $n - 1$ possibilities,
    \item the number of remaining \textbf{down} hall calls from floors \textbf{higher} than the current position: at most $n - 1$ possibilities,
    \item the number of remaining \textbf{up} hall calls from floors \textbf{lower} than the current position: at most $n - 1$ possibilities,
    \item the number of remaining \textbf{down} hall calls from floors \textbf{lower} than the current position: at most $n - 1$ possibilities,
    \item the number of remaining car calls in the current moving direction: at most $n - 1$ possibilities,
    \item the current position: $n$ possibilities,
    \item the current moving direction: 3 possibilities,
\end{itemize}

which make for a total of $(n-1)^4 \cdot (n - 1) \cdot n \cdot 3$ possible states. This is a vastly smaller amount than before the aggregation. In fact, it is not exponential in $n$ anymore. Such a size would allow for us to use tabular methods instead of value function approximators, given moderate sizes of $n$ and $m$. Note also that a large subset of states will (almost) never be visited under normal conditions, reducing the effective state space size even further.

% However, if the states are defined in too much detail, the number of states may become too large. 

\subsection{Action Set}

Now that we have the state space defined, we can move on to defining the actions we can take. What action the elevator is able to take will depend on the state of the system.

We define the actions as follows:

\begin{itemize}
    \item If the elevator is moving, it can either
        \begin{itemize}
            \item stop at the next floor, or
            \item continue past the next floor.
        \end{itemize}

    \item If the elevator is not moving, it can either:
        \begin{itemize}
            \item go up, or
            \item go down.
        \end{itemize}
\end{itemize}

The system considered is event-based. There are two main types of events. Events of the first type concern waiting times, such as passenger arrivals and transfers in and out of cars. Events of the second type are car arrival events, which are potential decision points for the elevator controllers. A car moving between floors generates a car arrival event when it reaches a point at which it must decide whether to stop at the next floor, or continue past it. Sometimes, the controlling agent is restricted to take a certain action. If a passenger wants to get off at the next floor, it has to stop at the next floor. A potential decision point is only considered a decision point if the choice of action is not constrained.

This approach equals that of \cite{crites_barto_1998, crites_barto_group_1998}, but differs from the approach used in \cite{walczak2006}. They suggest that instead of using the 2 aforementioned actions, an elevator chooses a floor to serve and commits to that action, under a couple of other constraints. This has wider applicability to practical elevator systems, where the stopping distance for an elevator moving at full speed is longer than half of the distance between floors. Another advantage is the ability to announce the arrival of the car several seconds beforehand. Passengers are given more time to go towards the entrance of the elevator. Although it has practical advantages, it makes the learning task more difficult. Instead of 2 possible actions, at most $n$ actions can be taken at each state.

In an attempt to build in some basic prior knowledge, we add additional restrictions on what actions can be taken:

\begin{itemize}
    \item When at the bottom and top floors, we can not choose the action go down and go up respectively. We cannot continue past the bottom and top floors.
    \item A car cannot turn until it has served all the car calls in its current direction.
    \item A car can stop at a floor only if there is a call from this floor or there
    is a car call to this floor.
    % \item A car cannot stop to pick up passengers at a floor if another car is already stopped there.
    \item Given a choice between moving up and down, it should prefer to move up (since
    the down-peak traffic tends to push the cars toward the bottom of the building).
\end{itemize}

\subsection{Performance Measures and Reward}

We need a way to measure the performance of the method we are applying. The goal is to minimize some function of passengers' waiting time. We consider the average passenger waiting time, which is generally considered a primary objective \cite{elevator_dynamics}. The waiting time of a passenger is defined as the time between the passenger's arrival at the floor and the passenger's entry into a car. Other possible performance measures are system time and the fraction of passengers waiting more than $T$ seconds, where $T$ is typically 60. System time is defined as the waiting time combined with the passenger's travel time.

In a Reinforcement Learning setting, reinforcement is usually formulated as maximizing some reward. In this case, however, it is more convenient to talk about minimizing costs of some sort. We want to define these costs such that minimizing them will lead to a lower average waiting time. The cost function is based on the definition of a reinforcement for an elevator system as in \cite{crites_barto_group_1998, crites_barto_1998}.

Since time between events is continuous, we consider instantaneous costs at continuous time moments. For $\tau \in [t_1, t_2]$, where $t_1$ is the time of taking an action for an elevator (i.e. deciding to continue past the next floor or stop) and $t_2$ is the time when the next decision is required, the reinforcement $r_\tau$ is defined as follows:

\begin{equation}
    r_\tau = \sum_{p\in P} (\tau - t_1 + w_p)^q
\end{equation}

where $q$ is any positive real, $P$ is the set of passengers waiting at time $t_2$ and $w_p$ is the amount of time a passenger $p\in P$ has already waited at time $t_1$. It should be noted that $w_p$ does not depend on $\tau$. Special care is needed to handle any passengers that begin or end waiting between $t_1$ and $t_2$, which is addressed in section \ref{sec:omniscient}

We choose to set $q=2$. A quadratic function has nice properties and makes it easy to compute the return integral. Furthermore, if $q$ were lower, very high waiting times would not be punished as much.

% Although the reinforcement value as above is passed to the learner that selected its action and time $t_1$ and completed its action at time $t_2$, the calculation includes the waiting times of all hall calls and car calls for other
% elevators as well.
\subsection{Simulation}

The controller learns to act from experience. This experience is generated by simulation. We simulate an hour of elevator elevator traffic and call that an episode. Every episode, the environment is reset to an initial state and passenger arrivals are generated. We can distinguish between two types of episodes, training and testing. In training episodes, we still take actions with suboptimal q values to keep exploring and estimating these values. In testing episodes, however, the training is done and only actions that are regarded as optimal will be taken. Further simulation specifics can be found in Appendix {\color{red}(??)}

\subsection{Traffic and Passenger Arrival}

It is important to take into account the traffic profile at a time. General building traffic profiles have been identified \cite{elevator_dynamics}. Four important profiles are up-peak, down-peak, inter-floor, and lunchtime. We will concern ourselves only with the down-peak traffic profile. Down-peak is a traffic pattern in which passengers are primarily moving down to the ground floor. An example is people going home at the end of a business day in an office building.

We model the arrival of passengers as a Poisson process with rates that fluctuate throughout the day. Table \ref{tab:rates} shows the expected number of passengers arriving at each floor in 5-minute intervals. Of these arriving passengers, 90\% are headed for the ground floor, while the rest is inter-floor traffic. 

% Please add the following required packages to your document preamble:
\begin{table}[h]
    \centering
    \caption{Arrival rates for the down-peak traffic profile.}
    \label{tab:rates}
    \begin{tabular}{@{}lllllllllllll@{}}
    \toprule
    Time & 00   & 05  & 10 & 15 & 20  & 25 & 30 & 35   & 40  & 45   & 50   & 55  \\ \midrule
    Rate & 0.25 & 0.5 & 1  & 1  & 4.5 & 3  & 2  & 1.75 & 4.5 & 1.25 & 0.75 & 0.5 \\ \bottomrule
    \end{tabular}
    \end{table}