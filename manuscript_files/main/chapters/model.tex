\section{Model of the System}

This section introduces the model of the elevator system studied here. The modelling approach is similar to the one used by \cite{walczak2006, crites_barto_1998}. We use this model in all further analysis.

\subsection{System Dynamics}
The system dynamics are parameterized as follows:

\begin{itemize}
    \item Number of floors: ?.
    \item Number of elevator cars: 1 {\color{red}for now?}. We consider only a single elevator to simplify the problem.
    \item Elevator floor time (time it takes for an elevator to pass a floor at full speed): 1.45 seconds.
    \item Elevator stop time (time it takes for the elevator to decelarate, open and close the doors, and accelerate again): 7.19 seconds.
    \item Elevator load time (the time it takes for a passenger to enter or exit the elevator): random variable from a 20\textit{th} order truncated Erlang distribution with a range from 0.6 to 6 seconds and a mean of 1 second.
    % \item Floor height. Set to ?? here.
    \item Capacity of the cars: 20 passengers.
    % \item Maximum number of passengers waiting for an elevator: 2 passengers.
\end{itemize}

Passengers can arrive on any floor, where they can press either the up or down button. We call this a hall call. After an elevator arrives at the passenger's floor, they get in and press a button to a floor in the same direction as the elevator. We call this a car call.

An elevator system can be very complex, with events such as passenger and elevator arrivals happening asynchronously. Even though time is continuous, it is still possible to model the elevator system as a discrete event system, where significant events happen at discrete times, with the time between them taking on a continuous value. In this case, the constant discount factor $\gamma$ used in most discrete-time reinforcement learning algorithms is inappropriate. Instead, we take into account the amount of time between events using a time varying discount factor \cite{bradtke_94}. This is also done to take into account the asynchronicity of events that is well suited to the elevator model. We define returns as the total discounted reward received over time:
\begin{equation}
    \int\limits_0^{\infty} e^{-\beta \tau} r_{\tau} d\tau
\end{equation}

where we integrate over continuous time, instead of the usual discrete-time reinforcement:

\begin{equation*}
    \sum_{t=0}^{\infty} \gamma^t r_t.
\end{equation*}

Here, $r_t$ is the immediate cost in the system at discrete time $t$, $r_\tau$ is the instantaneous cost at continuous time $\tau$ and $\beta > 0$ is a parameter used to control the rate of exponential decay. A system with a higher $\beta$ cares more about rewards that are closer in time. The environment is thusly modeled as a discrete-time Markov Decision Process with a finite number of states and actions. Using this approach, the states and actions can be fully specified.

The model parameters have been chosen such that the state space is finite and reasonably sized. This is of great importance when using algorithms in which it is necessary to store values for each state, so called tabular methods. If the state space is too large, running time and memory capacity can become intractable. We would need to approximate the value functions. 

% We use several assumptions to simplify the model further. We assume that the maximum number of passengers waiting for an elevator is 2. If another passenger arrives at a floor where 2 passengers are already waiting, we treat it as if there are still only 2 passengers waiting. We do this to limit the size of the state space.

It is not possible to observe the full state of the system. After a button is pressed, the elevator controller does not know if another passenger arrived after the first one. One way to deal with this is assuming \textit{omniscience}. We assume that in every state, the elevator controller knows how many passengers are waiting at a floor, when they arrived and where they are going. This can then be used to calculate the reinforcement signals. An important thing to note here is that it is not the controller that is receiving this extra information, it is the critic \textit{evaluating} the controller. This information is only important in the training phase. Once the controller is trained, it can be implemented in a real system without needing the extra knowledge.

Another possibility is to let the system learn using only information that would be available to the system on-line. The arrival times of passengers after the first would have to be estimated, so that expected costs can still be computed. We use omniscient reinforcements, since they are slightly more accurate and results will not differ by a large margin \cite{crites_barto_group_1998}. 

\subsection{Traffic and Passenger Arrival}

It is important to take into account the traffic profile at a time. General building traffic profiles have been identified \cite{elevator_dynamics}. Four important profiles are up-peak, down-peak, inter-floor, and lunchtime. We will concern ourselves only with the down-peak traffic profile. Down-peak is a traffic pattern in which passengers are primarily moving down to the ground floor. An example is people going home at the end of a business day in an office building. {\color{red} We will assume every arriving passenger wants to go to the ground floor. (maybe)}

We model the arrival of passengers as a Poisson process with rate parameter $\lambda$ being the expected number of people arriving at a floor each minute. The rate can vary across floors {\color{red} and time (maybe)}. We set it to ??. For every floor, passenger interarrival time is  drawn from an exponential distribution with rate $\lambda$.

\subsection{State Space}

An elevator controller maps state information to decisions. The definition of the state is therefore of great importance. It needs to properly reflect the current state of the system, measured by quantities that can be practically observed. This includes:

\begin{itemize}
    \item the state of hall calls and their waiting times,
    \item awaiting car calls and their waiting times for all elevators,
    \item position of all elevators,
    \item moving direction of all elevators,
    \item velocity of all elevators,
    \item calls to which particular elevators are allocated.
\end{itemize}

If we assume the state is defined as above, it is possible to estimate the size of the resulting state space. Let $n$ be the number of floors and $m$ the number of elevators {\color{red} to notation?}, the size of the state space can be estimated as follows:

\begin{itemize}
    \item at most $n - 1$ up hall calls: $2^{n-1}$ possibilities,
    \item at most $n - 1$ down hall calls: $2^{n-1}$ possibilities,
    \item at most $n$ car calls for each elevator: $2^{n}$ possibilities,
    \item possible locations for each elevator: $n$ possibilities,
    \item possible elevator directions for each elevator: $3$ possibilities (up, down or stopped),
\end{itemize}

which gives a state space size of $2^{n - 1} \cdot 2^{n - 1} \cdot 2^{n} \cdot n \cdot 3$ in a situation where each elevator is considered as a seperate learner. The amount of states rises tremendously in the number of floors. The problem already gets computationally intractable for a relatively few number of floors. To combat this and further reduce the size of the state space, we aggregate the states into a more compact representation, while hopefully keeping the most relevant information. Instead of receiving information regarding exactly which hall and car calls are active, the controller only receives: 

\begin{itemize}
    \item the number of up and down awaiting hall calls higher and lower than the elevator's current position,

    \item the number of car calls to floors in the current moving direction.
\end{itemize}

Taking into account this particular aggregation of states, we can recalculate the size of the state space as follows:

\begin{itemize}
    \item the number of remaining \textbf{up} hall calls from floors \textbf{higher} than the current position: at most $n - 1$ possibilities,
    \item the number of remaining \textbf{down} hall calls from floors \textbf{higher} than the current position: at most $n - 1$ possibilities,
    \item the number of remaining \textbf{up} hall calls from floors \textbf{lower} than the current position: at most $n - 1$ possibilities,
    \item the number of remaining \textbf{down} hall calls from floors \textbf{lower} than the current position: at most $n - 1$ possibilities,
    \item the number of remaining car calls in the current moving direction: at most $n - 1$ possibilities,
    \item the current position: $n$ possibilities,
    \item the current moving direction: 3 possibilities,
\end{itemize}

which make for a total of $(n-1)^4 \cdot (n - 1) \cdot n \cdot 3$ possible states. This is a vastly smaller amount than before the aggregation. In fact, it is not exponential in $n$ anymore. Such a size would allow for us to use tabular methods instead of value function approximators, given moderate sizes of $n$ and $m$. Note also that a large subset of states will (almost) never be visited under normal conditions, reducing the effective state space size even further. {\color{red} probabilistic analysis number of states visited on average}


% However, if the states are defined in too much detail, the number of states may become too large. 

\subsection{Action Set}

Now that we have the state space defined, we can move on to defining the actions we can take. What action the elevator is able to take will depend on the state of the system.

We define the actions as follows:

\begin{itemize}
    \item If the elevator is moving, it can either
        \begin{itemize}
            \item stop at the next floor, or
            \item continue past the next floor.
        \end{itemize}

    \item If the elevator is not moving, it can either:
        \begin{itemize}
            \item go up, or
            \item go down.
        \end{itemize}
\end{itemize}

An elevator approaching the next floor will create an arrival event, upon which it has to decide if it will stop at the next floor and decelerate or if it will continue past the next floor. This approach equals that of \cite{crites_barto_1998, crites_barto_group_1998}, but differs from the approach used in \cite{walczak2006}. They suggest that instead of using the 2 aforementioned actions, an elevator chooses a floor to serve and commits to that action, under a couple of other constraints. This has wider applicability to practical elevator systems, where the stopping distance for an elevator moving at full speed is longer than half of the distance between floors. Although it has practical advantages, it makes the learning task more difficult. Instead of 2 possible actions, roughly $n$ actions can be taken at each state.

There are additional restrictions on what actions can be taken:

\begin{itemize}
    \item When at the bottom and top floors, we can not choose the action go down and go up respectively. We cannot continue past the bottom and top floors.
    \item We can not turn in a single action. If the current direction is up, for example, we have to first stop the elevator before we can go down.
    \item A car cannot turn until it has served all the calls in its current direction.
    \item A car can stop at a floor only if there is a call from this floor or there
    is a car call to this floor.
    \item A car cannot stop to pick up passengers at a floor if another car is already stopped there.
    \item Given a choice between moving up and down, it should prefer to move up (since
    the down-peak traffic tends to push the cars toward the bottom of the building).
\end{itemize}

\subsection{Performance Measures and Reward}

We need a way to measure the performance of the method we are applying. The goal is to minimize some function of passengers' waiting time. We consider the average passenger waiting time, which is generally considered a primary objective \cite{elevator_dynamics}. The waiting time of a passenger is defined as the time between the passenger's arrival at the floor and the passenger's entry into a car. Other possible performance measures are system time and the fraction of passengers waiting more than $T$ seconds, where $T$ is typically 60. System time is defined as the waiting time combined with the passenger's travel time.

In a Reinforcement Learning setting, reinforcement is usually formulated as maximizing some reward. In this case, however, it is more convenient to talk about minimizing costs of some sort. We want to define these costs such that minimizing them will lead to a lower average waiting time. The cost function is based on the definition of a reinforcement for an elevator system as in \cite{crites_barto_group_1998, crites_barto_1998}.

Since time between events is continuous, we consider instantaneous costs at continuous time moments. For $\tau \in [t_1, t_2]$, where $t_1$ is the time of taking an action for an elevator (i.e. deciding to continue past the next floor or stop) and $t_2$ is the time when the next decision is required, the reinforcement $r_\tau$ is defined as follows:

\begin{equation}
    r_\tau = \sum_p (\tau - t_1 + w_p)^2
\end{equation}

where $w_p$ is the amount of time each passenger $p$ waiting at time $t_2$ has already waited at
time $t_1$. Special care is needed to handle any passengers that begin or end waiting between
$t_1$ and $t_2$. {\color{red} See Section ??}

% Although the reinforcement value as above is passed to the learner that selected its action and time $t_1$ and completed its action at time $t_2$, the calculation includes the waiting times of all hall calls and car calls for other
% elevators as well.