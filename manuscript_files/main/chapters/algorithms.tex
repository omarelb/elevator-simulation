\section{The Algorithm}

There are various methods and philosophies to solving Reinforcement Learning problems. The basic idea for most algorithms is to estimate reward values for states. These values are then used to make sure we take actions so that we stay in high-value states. $Q$ is a function that maps state-action pairs $(s, a)$ to a numerical value. This value represents the expected total discounted return after taking action $a$ in state $s$. These values are then used to approximate the optimal policy $\pi_*$.  

\subsection{$Q$-learning}

We use the Q-learning algorithm \cite{watkins_89}, which can be categorized as an off-policy Temporal Difference (TD) control method \cite{sutton_barto_2012}, where the Q-update is generally defined by
\begin{align}
    Q(s_t, a_t) &\leftarrow Q(s_t, a_t) + \alpha \left[ r_{t + 1} + \gamma \max\limits_a Q(s_{t + 1}, a) - Q(s_t, a_t) \right]\\
    &= (1 - \alpha) Q(s_t, a_t) + \alpha \left[r_{t + 1} + \gamma \max\limits_a Q(s_{t+1}, a) \right]. \label{eq:q_update_discrete}
\end{align}
It is, in essence, a stochastic approximation to the Bellman Optimality equations.  %\ref{eq:bellman}
The advantage of this method is that we do not need to fully specify a model of the environment. We do not need to know, for instance, state transition probabilities and the distribution of reinforcements. These can be difficult to specify in an environment as complex as the elevator system. Instead, we use a simulator to generate episodes.

Q-learning is an off-policy method, because it does not necessarily learn the same policy that is being followed. The learned action-value function, $Q$, directly approximates the optimal action-value function $q_*$, independent of the policy being followed. The policy being followed still determines which state-action pairs visited and updated. However, all that is required for correct convergence is that all pairs continue to be updated. Under this assumption and a variant of the usual stochastic approximation conditions on the sequence
of step-size parameters $\alpha_t$, $Q$ has been shown to converge to $q_*$ with probability 1.

The $Q$-learning algorithm is given below.

\begin{center}
    \begin{minipage}{12cm}
    \begin{algorithm}[H]
        \caption{Q-learning (off-policy TD control) for estimating $\pi \approx \pi_*$}
        \label{alg:qlearning}
    \begin{algorithmic}[1]
    
    \STATE Initialize $Q(s, a)$, for all $s \in S$, $a \in A(s)$, arbitrarily, and $Q(terminal-state, \cdot) = 0$
    \REPEAT
    \STATE Initialize $S$
    \REPEAT
    \STATE Choose $A$ from $S$ using policy derived from $Q$
    \STATE Take action $A$, observe $R$, $S'$
    % \STATE Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
    \STATE $Q(S, A) \leftarrow (1 - \alpha) Q(S, A) + \alpha \left[R + \gamma \max\limits_a Q(S', a) \right]$
    \STATE $S \leftarrow S'$
    \UNTIL ($S$ is terminal)
    \UNTIL {episodes end}
    \end{algorithmic}
    \end{algorithm}
    \end{minipage}
\end{center}

\subsection{Function Updates}
We need to modify Algorithm \ref{alg:qlearning} to meet our requirements. We now minimize over costs instead of maximizing over rewards. Since we are acting in continuous time, it is necessary to change the $Q$ function update rule described in (\ref{eq:q_update_discrete}). For a time interval $[t_1, t_2]$ as considered above, the update rule for the $Q$-value for state $s_{t_1}$ and action $a_{t_1}$ is given at time $t_2$ by

\begin{equation}
    Q(s_{t_1}, a_{t_1}) \leftarrow     
    (1 - \alpha) Q(s_{t_1}, a_{t_1}) + \alpha \left[r_{[t_1,t_2]} + e^{-\beta(t_2 - t_1)} \min\limits_a Q(s_{t_2}, a) \right] \label{eq:q_update_continuous}
\end{equation}

where the total discounted cost for time interval $[t_1, t_2]$, $r_{[t_1, t_2]}$, is given by

\begin{equation}
    \int_{t_1}^{t_2} e^{-\beta(\tau - t_1)} \sum_{p\in P}(\tau - t_1 + w_p)^2 d\tau. \label{eq:cost_reinforcement_int}
\end{equation}

which can be solved by parts (see Appendix \ref{sec:solving_integral}) to yield
\begin{equation}
    \sum_{p\in P} \left(\frac{2}{\beta^3} + \frac{2w_p}{\beta^2} + \frac{w_p^2}{\beta} - e^{-\beta(t_2 - t_1)} \left[ \frac{2}{\beta^3} + \frac{2(w_p + t_2 - t_1)}{\beta^2} + \frac{(w_p + t_2 - t_1)^2}{\beta} \right]\right). \label{eq:cost_reinforcement_sum}
\end{equation}

\label{sec:omniscient}
\subsection{Calculating Omniscient Reinforcements}
% The amount of cost accumulated between passenger arrival events is the same for all cars since they share the same objective function, but the amount of cost each car accumulates between its decisions is different since the cars make their decisions at different times. Therefore, each car $i$ has an associated storage location, $R[i]$, where the total discounted cost it has incurred since its last decision (at time $d[i]$) is accumulated.
We have to be careful in doing the accounting of reinforcements. To deal with discontinuities due to passenger arrivals and transfers, we update the cost between decisions incrementally. In the omniscient reinforcement scheme, which we are using, accumulated cost for the car is updated after every passenger arrival event (when a passenger arrives at a floor), passenger transfer event (when a passenger gets on or off of a car), and when a control decision is made. The car has a cost storage $R$, where the total discounted cost it has incurred since its last decision, at time $d$, is accumulated. At the time of each aforementioned event, the following computations are performed: Let $t_0$ be the time of the last event and $t_1$ the time of the current event. For each passenger $p$ that has been waiting between $t_0$ and $t_1$, let $w_0(p)$ and $w_1(p)$ be the total time that passenger $p$ has waited at $t_0$ and $t_1$ respectively. Then for each car $i$,

\begin{equation}
   \Delta R = \sum_{p\in P} e^{-\beta (t_0 - d)} \left[\frac{2}{\beta^3} + \frac{2w_0(p)}{\beta^2} + \frac{w_0^2(p)}{\beta}  \right] - e^{-\beta(t_1 - d)} \left[ \frac{2}{\beta^3} + \frac{2w_1(p)}{\beta^2} + \frac{w_1^2(p)}{\beta} \right]. \label{eq:cost_reinforcement_partial}
\end{equation}

To keep reinforcements from blowing up, they are scaled down by a factor $10^6$.

\label{sec:action_selection}
\subsection{Making Decisions and Updating Q-Values}

A car generates a car arrival event when it is moving between floors and reaches a point at which it must decide to either continue or stop at the next floor. Sometimes, the controller is constrained to a single action, e.g. stopping when a passenger wants to get off at the next floor. In such a case, the event is not seen as a decision point and the $Q$-values will not be updated. The algorithm used by the agent for making decisions and updating its $Q$-value estimates is as follows:
\begin{enumerate}
    \item At time $t_1$, observing state $s_{t_1}$, the car arrives at a decision point. It selects an action $a$ according to the Boltzmann distribution over its $Q$-value estimates:
    \[
        \Pr(stop) = \frac{e^{Q(s_{t_1},continue)/T}}{e^{Q(s_{t_1},continue)/T} + e^{Q(s_{t_1},stop)/T}}
    \]
    where $T$ is a positive ``temperature'' parameter that is annealed (decreased) during learning. $T$ controls the amount of exploration in the selection of actions. At the beginning of learning, when the $Q$-value estimates are very inaccurate, higher values of $T$ are used, which give almost equal probabilities to each action. Later in learning, when the $Q$-value estimates are more accurate, lower values of $T$ are used, which give higher probabilities to actions that are thought to be superior, while still allowing some exploration to gather more information about the other actions. It is important to consider how to decrease $T$ over time, which is discussed in Section \ref{sec:annealing_schedules}.

    \item Let the next decision point for the car be at time $t_2$ in state $s_{t_2}$. After the car has updated its $R$ value as described above, it adjusts its estimate of $Q(s_{t_1}, a)$ toward the following target value:

    \[
    R + e^{-\beta(t_2 - t_1)} \min\limits_{\{stop, cont\}} Q(s_{t_2}, \cdot).    
    \]

    \item
    Let $s_{t_1} \leftarrow s_{t_2}$ and $t_1 \leftarrow t_2$. Go to step 1.

\end{enumerate}
