\section{Results and Analysis}

\subsection{Annealing Schedules}
\label{sec:annealing_schedules}

In section \ref{sec:omniscient}, a strategy for choosing actions was outlined. We sample from the Boltzmann distribution over our $Q$-values with temperature parameter $T$ controlling the randomness over time. As training progresses, $T$ is annealed according to the following schedule:
\[
T = 2 \cdot factor^h
\]
where $factor$ is a positive number less than 1 that determines how quickly the temperature decreases, while $h$ is the number of episodes simulated until now. The number of episodes to train for depends on $factor$. We stop training only when the temperature reaches a predetermined (small) number $T_{end}$. Rewriting the above equation in terms of number of hours yields 
\[
h = \log_{factor}\left(\frac{T}{2}\right).
\]
Assuming $T_{end} = 0.001$ and $factor = 0.9995$, we would thus train for $\log_{0.9995}(0.0005) \approx 15198$ episodes.

Not only do we anneal temperature, we also anneal update step-size parameter $\alpha$ according to the following schedule:
\[
\alpha = 0.01 \cdot 0.99975^h.
\]
